---
title: "Small team statistics"
author:
  - George G. Vega Yon
  - Kayla de la Haye
date: "October 1, 2018"
output: pdf_document
bibliography: bibliography.bib
header-includes:
  - \usepackage{hyperref}
  - \usepackage{arev}
  - \usepackage[T1]{fontenc}
---

```{r knitr-setup, echo=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning = FALSE, fig.align = 'center', out.width = '.7\\linewidth')
```

# Exponential Random Graph Models

## Formals

The distribution of $\mathbf{Y}$ can be parameterized in the form

$$
\Pr\left(\mathbf{Y}=\mathbf{y}|\theta, \mathcal{Y}\right) = \frac{\exp{\theta^{\mbox{T}}\mathbf{g}(\mathbf{y})}}{\kappa\left(\theta, \mathcal{Y}\right)},\quad\mathbf{y}\in\mathcal{Y}
\tag{1}
$$

Where $\theta\in\Omega\subset\mathbb{R}^q$ is the vector of model coefficients and $\mathbf{g}(\mathbf{y})$ is a *q*-vector of statistics based on the adjacency matrix $\mathbf{y}$.


*   Model (1) may be expanded by replacing $\mathbf{g}(\mathbf{y})$ with $\mathbf{g}(\mathbf{y}, \mathbf{X})$ to allow for additional covariate information $\mathbf{X}$ about the network. The denominator,
    
    
    $$
    \kappa\left(\theta,\mathcal{Y}\right) = \sum_{\mathbf{z}\in\mathcal{Y}}\exp{\theta^{\mbox{T}}\mathbf{g}(\mathbf{z})}
    $$
    
*   Is the normalizing factor that ensures that equation (1) is a legitimate probability distribution.

*   Even after fixing $\mathcal{Y}$ to be all the networks that have size $n$, the size of $\mathcal{Y}$ makes this type of models hard to estimate as there are $N = 2^{n(n-1)}$ possible networks! [@Hunter2008]

## How does ERGMs look like (in R at least)

```r
network ~ edges + nodematch("hispanic") + nodematch("female") +
  mutual +  esp(0:3) +  idegree(0:10)
```

Here we are controlling for: 

*   `edges`: Edge count,
*   `nodematch(hispanic)`: number of homophilic edges on race,
*   `nodematch(female)`: number of homophilic edges on gender,
*    `mutual`: number of reciprocal edges,
*   `esp(0:3)`: number of shared parterns (0 to 3), and
*   `indegree(0:10)`: indegree distribution (fixed effects for values 0 to 10)


[See @Hunter2008].

# Example with precision

For each team $T$, we defined the following statistic:

$$
S_T\equiv 1 - \frac{1}{n(n-1)}\sum_{i \in N}H(G_i,G_T)
$$

Where $H$ is the hamming distance, $G_i$ is $i$'s Cognitive Social Structure, and $G_T$ is the true network.

The statistic is normalized so that it lies wihin 0 and 1, with 0 been complete missmatch, and 1 perfect match.

# Simulation process

For each set of experiments, generate $N$ teams by doing:

1.  Draw a random graph of size $n_i$ from a bernoulli distribution with parameter $p_i$,
call it $G_i$.

2.  Generate $n_i$ other graphs by permuting $G_i$ with different levels of accuracy $a_{ij}$

3.  Generate $Y_i \sim \text{Beta}\left(\exp{\left(\theta^\text{t}X_i\right)}, 1.5\right)$,
where $X_i$ is a vector of team level statistics, including $a_i = n_i^{-1}\sum_j a_{ij}$,
the average level of accuracy. The resulting value $Y_i$ will be between 0 and 1.

Once all $N$ teams have been simulated, estimate the model using MLE

# Monte Carlo Experiments

```{r setup, echo=FALSE}

library(magrittr)
library(stats4)

set.seed(65454)

source("beta_mle.R")

n_sims  <- 200
n_teams <- 50
n       <- replicate(n_sims, sample(c(3, 4, 5), n_teams, TRUE), simplify = FALSE)
dens    <- replicate(n_sims, runif(n_teams), simplify = FALSE)
prec    <- replicate(n_sims, runif(n_teams), simplify = FALSE)

theta   <- replicate(n_sims, rnorm(3), simplify=FALSE)
X       <- lapply(n, function(n0) {
  cbind(
    X1 = rbinom(n_teams, n0, .5)/n0,
    X2          = rnorm(n_teams)
    )
  })
```

```{r example-1, echo=FALSE}
# Simple example
z <- sim_experiment(
  n     = n[[2]],
  dens  = dens[[2]],
  prec  = prec[[2]],
  X     = X[[2]],
  theta = c(-2, 1, .5)
  )

# Extracting the data
d <- cbind(
  h = sapply(z, "[[", "prec_hat"),
  y = sapply(z, "[[", "response"),
  X = X[[1]]
)

ans <- beta_mle(d[,"y"], d[,-2])
knitr::kable(summary(ans)@coef, caption = "Estimates from simulated data")

# Correlation
hist(d[,"y"], xlab = "Response", main="Simulated Experiment (50 teams)")

plot(
  x = sapply(z, "[[", "prec"),
  y = sapply(z, "[[", "prec_hat"),
  xlab = "True Precision",
  ylab = "Estimated Precision\n(hamming distance)"
)
```


```{r data-generating-process, cache=TRUE}
ans <- parallel::mcmapply(
  sim_experiment,
  n = n, dens=dens, prec=prec, X=X, theta=theta, mc.cores = 8,
  SIMPLIFY = FALSE
  )
```

```{r mle, cache=TRUE}
# Estimating models ------------------------------------------------------------
mles0 <- parallel::mcmapply(function(dat, x) {
  
  # Extracting the data
  d <- cbind(
    y = sapply(dat, "[[", "response"),
    hamming_distance = sapply(dat, "[[", "prec_hat"),
    X = x
  )
  
  beta_mle(d[,"y"], d[,-1])
  
}, dat = ans, x=X, mc.cores=8, SIMPLIFY=FALSE)

mles1 <- parallel::mcmapply(function(dat, x) {
  
  # Extracting the data
  d <- cbind(
    y = sapply(dat, "[[", "response"),
    group_size = sapply(dat, "[[", "n"),
    X = x
  )
  
  beta_mle(d[,"y"], d[,-1])
  
}, dat = ans, x=X, mc.cores=8, SIMPLIFY=FALSE)


mles2 <- parallel::mcmapply(function(dat, x) {
  
  # Extracting the data
  d <- cbind(
    y = sapply(dat, "[[", "response"),
    hamming_distance = sapply(dat, "[[", "prec_hat"),
    group_size = sapply(dat, "[[", "n"),
    X = x
  )
  
  beta_mle(d[,"y"], d[,-1])
  
}, dat = ans, x=X, mc.cores=8, SIMPLIFY=FALSE)
```

```{r analysis-of-power}
# Computing pvalues ------------------------------------------------------------

pvals0 <- lapply(mles0, function(model) {
  tryCatch(calc_pval(model@coef, model@vcov), error =function(e) NULL)
  })
pvals0 <- do.call(rbind, pvals0)
boxplot(1 - pvals0, main="Power")

pvals1 <- lapply(mles1, function(model) {
  tryCatch(calc_pval(model@coef, model@vcov), error =function(e) NULL)
  })
pvals1 <- do.call(rbind, pvals1)
boxplot(1 - pvals1, main="Power")

pvals2 <- lapply(mles2, function(model) {
  tryCatch(calc_pval(model@coef, model@vcov), error =function(e) NULL)
  })
pvals2 <- do.call(rbind, pvals2)
boxplot(1 - pvals2, main="Power")

```


# References