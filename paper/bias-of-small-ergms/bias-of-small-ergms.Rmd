---
title: "Bias in Little ERGMs"
author:
  - George G. Vega Yon, M.Sc.
  - Kayla de la Haye, Ph.D.
date: "October 5, 2018"
output: html_document
---

# Introduction

Exponential Random Graph Models (ERGMs)...

Currently, estimation methods for ERGMs have been developed focused on rather medium to large (but not very large) networks spanning from 20 to a couple of thousands vertices. Moreover, modern estimation techniques are based on MCMC-MLE methods, which provides sound statistical theory for doing inference on parameter estimates thanks to the asymptotic normality of MLEs.

The problem with this is that, when it comes to small network, not much of this can be used since asymptotic theory won't work here. While the usage of the Cramer-Rao Lower Bound allows us to calculate the exact Variance of our MLEs without relying on asymptotics, we still need to know what is the exact distribution of the MLEs under small sample estimates.

In this paper, we focus on two things: Probability of Type II error, and observed bias of what we call Little ERGMs (lERGMs).

# Estimation of lERGMs

As a difference from the more common scenario, estimating ERGMs with few nodes makes it possible to use the exact likelihood function, which allows us to use MLE directly without having to deal with simulation processes, in other words, the problem reduces to just finding the set of parameters that maximizes the log-likelihood of a given graph, which is a simple optimization problem.

# Monte Carlo Simulations

Regardless of the population parameters, we can always estimate lERGMs for all $2^{n(n-1)}$ possible networks. This saves us a significant amout of time since for our simulations it implies that we can reuse those estimates and calculate bias/coverage probability/and type II errors by just updating the vector of probabilities associated with each network under the corresponding set of parameters. 

So, the first step is to estimate the following set of models:

$$
\hat\theta_y = {\arg\max}_{\theta\in\Theta}\log\Pr\left(Y=y|\theta, g(y)\right),\quad\forall y\in\mathcal{Y}
$$
Given the previous set of parameter estimates, we do the following

1.  Sample $\theta_b \in \Theta$

2.  Calculate the vector of probabilities $P_b = \{P_b(y)\}_{y\in\mathcal{Y}}= \{\Pr(Y=y|\theta_b, g(y))\}_{y\in\mathcal{Y}}$

3.  We can compute the bias, coverage probability, and Type I/Type II error rates as follow:
    
    $$
    \begin{array}{ll}
    \mbox{Bias:} & \sum_{y\in \mathcal{Y}}P_b(y)\left(\hat\theta_{y} - \theta_b\right) \\
    \mbox{Coverage:} & \sum_{y\in \mathcal{Y}} P_b(y)\mathbf{1}\left(-|\hat\theta_y- \theta_b| < \Pr^{-1}(.025) \right)\\
    \mbox{Type II Error:} & \sum_{y\in\mathcal{Y}}P_b(y)\mathbf{1}\left(\mbox{p-value} > 0.05\right)
    \end{array}
    $$
      
    Where $\Pr^{-1}(.025)$ is the quantile for the distribution of $\hat\theta_y$
    
# Simulations

```{r simulation-parameters}
library(lergm)
library(ergm)
nsim <- 5000
n    <- 4
NCORES <- 2
```

```{r estimating-for-all}
all_thetas <- function(n) {
  # Generating the power set of networks
  G <- powerset(n, mc_cores = NCORES)
  
  # Computing statistics
  S <- parallel::mclapply(G, function(g) summary(g ~ edges + balance + mutual),
                          mc.cores = NCORES)
  S <- do.call(rbind, S)
  
  S <- list(
    weights = rep(1, length(G)),
    statmat = S
  )
  
  # Estimating 2^(n(n-1)) lergms
  thetas <- parallel::mclapply(1:length(G), function(i) {
    
    Si <- S
    Si$statmat <- Si$statmat - 
      matrix(Si$statmat[i,], ncol=ncol(Si$statmat), nrow = nrow(Si$statmat),
             byrow=TRUE)
    
    coef(lergm::lergm(G[[i]] ~ edges + balance + mutual, stats = Si))
    
  }, mc.cores = NCORES)
  
}

z <- all_thetas(4)
```


```{r, echo=FALSE}
knitr::opts_chunk$set(cache = FALSE, eval=FALSE)
```





```{r data-generating-process}
net_sampler <- function(theta, n) {
  
  # Generating the power set of networks
  G <- powerset(n, mc_cores = NCORES)
  
  # Computing statistics
  S <- parallel::mclapply(G, function(g) summary(g ~ edges + balance + mutual),
                          mc.cores = NCORES)
  S <- do.call(rbind, S)
  
  # Centering at about the first
  s <- summary(G[[1]] ~ edges + balance + mutual)
  S <- S - matrix(s, ncol=3, nrow=nrow(S), byrow = TRUE)
  
  Pr <- lergm:::exact_loglik(theta, rep(1, nrow(S)), S)[1]
  Pr <- exp(S %*% theta + Pr)
  
  function(m) {
    
    idx <- sample.int(length(G), m, TRUE, prob = Pr)
    
    structure(
      G[idx],
      stats = S
    )
    
  }
  
  
}

set.seed(1)
params <- matrix(rnorm(1e3*3), ncol=3)
G <- net_sampler(params[1,], n = n)
```

```{r parameter-estimates}
Gsample <- G(nsim)
Ssample <- list(
  weights = rep(1, nrow(attr(Gsample, "stats"))),
  statmat = attr(Gsample, "stats")
)

sim1 <- parallel::mclapply(Gsample, function(g) {
  
  coef(lergm(g ~ edges + balance + mutual, stats = Ssample))
  
}, mc.cores = 2)
```

```{r bias}

sim1 <- do.call(rbind, sim1)

boxplot(
  matrix(params[1,], ncol=3, nrow=nsim) - sim1, 
  main = "Estimation Biases",
  xlab = "Parameter"
  )

```



